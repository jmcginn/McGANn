%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    INSTITUTE OF PHYSICS PUBLISHING                                   %
%                                                                      %
%   `Preparing an article for publication in an Institute of Physics   %
%    Publishing journal using LaTeX'                                   %
%                                                                      %
%    LaTeX source code `ioplau2e.tex' used to generate `author         %
%    guidelines', the documentation explaining and demonstrating use   %
%    of the Institute of Physics Publishing LaTeX preprint files       %
%    `iopart.cls, iopart12.clo and iopart10.clo'.                      %
%                                                                      %
%    `ioplau2e.tex' itself uses LaTeX with `iopart.cls'                %
%                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% First we have a character check
%
% ! exclamation mark    " double quote  
% # hash                ` opening quote (grave)
% & ampersand           ' closing quote (acute)
% $ dollar              % percent       
% ( open parenthesis    ) close paren.  
% - hyphen              = equals sign
% | vertical bar        ~ tilde         
% @ at sign             _ underscore
% { open curly brace    } close curly   
% [ open square         ] close square bracket
% + plus sign           ; semi-colon    
% * asterisk            : colon
% < open angle bracket  > close angle   
% , comma               . full stop
% ? question mark       / forward slash 
% \ backslash           ^ circumflex
%
% ABCDEFGHIJKLMNOPQRSTUVWXYZ 
% abcdefghijklmnopqrstuvwxyz 
% 1234567890
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\documentclass[12pt]{iopart}
\bibliographystyle{iopart-num_custom}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{comment}
\usepackage{acronym}
\usepackage{standalone}
\usepackage{academicons}
\usepackage{scalerel}
\usepackage{layouts}
\usepackage{import}
%\usepackage{subfig}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,angles,quotes,shapes.geometric, arrows,calc}
\usetikzlibrary{svg.path}
\expandafter\let\csname equation*\endcsname\relax 
\expandafter\let\csname endequation*\endcsname\relax 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}


\usepackage{graphicx}
\usetikzlibrary{decorations.pathreplacing,angles,quotes,shapes.geometric, arrows,calc}
\usepackage{wrapfig, blindtext}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}


\usepackage[mathlines]{lineno}% Enable numbering of text and display math
\linenumbers\relax % Commence numbering lines


\newcommand{\gguide}{{\it Preparing graphics for IOP Publishing journals}}
%Uncomment next line if AMS fonts required
%\usepackage{iopams}  
\newcommand{\jordan}[1]{\textbf{\textcolor{red}{JORDAN: #1}}}
\newcommand{\siong}[1]{\textbf{\textcolor{blue}{SIONG: #1}}}
\newcommand{\chris}[1]{\textbf{\textcolor{green}{CHRIS: #1}}}
\newcommand{\michael}[1]{\textbf{\textcolor{orange}{MICHAEL: #1}}}
\newcommand{\dcc}{LIGO-PXXXXXXX}
%\input{tag.tex}
% Command for ORCID id's
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
\begin{tikzpicture}[yscale=-1,transform shape]
\pic{orcidlogo};
\end{tikzpicture}
}{|}}}}

\usepackage{hyperref} %<--- Load after everything else

\begin{document}

\title{Generalised gravitational burst generation with Generative Adversarial Networks}

\author{
    J. McGinn \orcidicon{0000-0000-0000-0000},
    C. Messenger \orcidicon{0000-0001-7488-5022},
    I.S. Heng \orcidicon{0000-0000-0000-0000},
    M. J. Williams \orcidicon{0000-0003-2198-2974}
}

\address{University of Glasgow, Physics \& Astronomy Department, Glasgow G12 8QQ, UK}
%\ead{jordan.mcginn@glasgow.ac.uk}
\vspace{10pt}
%\begin{indented}
%\item[]\commitDATE\\\mbox{\small \commitID}\\\mbox{\dcc}
%\end{indented}

\begin{abstract}
The next generation of Gravitational wave detectors will accelerate the number
of ac{GW} detections such that we can gain new in site into the
physics behind the sources causing the phenomena. Numerical simulations and
matched filtering are the standard for detecting ac{GW}s for known
sources such as binary-black hole mergers. There are other sources of ac{GW}s
that remain elusive to standard modelling techniques and are expected to be
detectable. Here we construct a unmodelled burst generation scheme using
Generative adversarial networks - a powerful class of machine learning.
\chris{time to add a real abstract}
\end{abstract}

%
% Uncomment for keywords
%\vspace{2pc}
%\noindent{\it Keywords}: XXXXXX, YYYYYYYY, ZZZZZZZZZ
%
% Uncomment for Submitted to journal title message
%\submitto{\JPA}
%
% Uncomment if a separate title page is required
%\maketitle
% 
% For two-column output uncomment the next line and choose [10pt] rather than [12pt] in the \documentclass declaration
%\ioptwocol
%

\acrodef{GW}[GW]{gravitational wave}
\acrodef{CBC}[CBC]{compact binary coalescence} 
\acrodef{ML}[ML]{machine learning}
\acrodef{AI}[AI]{artificial intelligence}
\acrodef{CNN}[CNN]{convolutional neural network}
\acrodef{GAN}[GAN]{generative adversarial network}
\acrodef{CGAN}[CGAN]{conditional generative adversarial network}
\acrodef{ACGAN}[ACGAN]{auxilliary conditional generative adversarial network}
\acrodef{DCGAN}[DCGAN]{deep convolutional generative adversarial network}
\acrodef{CNN}[CNN]{convolutional neural networks}
\acrodef{BBH}[BBH]{binary black hole}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%textwidth in inches: \printinunitsof{in}\prntlen{\textwidth}
\begin{comment}
\begin{itemize}
\item Need to introduce GWs - the current state of the field e.g. detections
and LVC papers \ding{51}
\item Introduce burst searches - what's the point of burst searches \ding{51} - lots of references 
\item Discuss the family of burst waveforms currently used and why - not in detail, just
an introduction \ding{51}
\item Introduce ML techniques in GWs \ding{51} - lots of references
\item What this paper does on GANs in 1 paragraph \ding{51}
\item Describe the structure of the paper 
\end{itemize}
\end{comment}

% introduce \ac{GW} astrophsyics
%
\ac{GW} astronomy is now an established field, starting with the first
detection of a binary black hole merger~\cite{Abbott2016} in September 2015.
Following this, the first and second observations runs (O1 and O2) of Advanced
LIGO and Advanced Virgo~\cite{Prospects-dets, AdvLIGO, AdvLIGO2, AdvVIRGO}
reported several more \ac{CBC} mergers~\cite{Abbott2016a, Abbott2017,
Abbott2017a, Abbott2017b}. On 17th August 2017 a binary neutron star merger was
observed alongside its electromagnetic counterpart for the first time, giving
rise to multi-messenger ac{GW} astronomy. 

% introduce burst searches
%
With these successes and continued upgrades to the detectors, further
detections of \acp{CBC} are expected to be commonplace in future advanced
detcetor observation runs. Another group of \ac{GW} signals that has thus far
been undetected is \ac{GW} ``bursts". \ac{GW} bursts are classed as transient
signals of typically short duration ($<$ 1s) whose waveforms are not accurately
modelled or are complex to reproduce. Astrophysical sources for such transients
include: Core collapse supernova~\cite{Fryer_2003}, Pulsar
glitches~\cite{Andersson_2001}, Neutron star post-mergers~\cite{Baiotti_2007}
and other as-yet unexplained astrophysical phenomena. 

% more details on burst searches
%
\ac{GW} searches for modelled signals so far have used a process called
matched-filtering,~\cite{Owen1998}, where a large template bank of possible
\ac{GW} waveforms are compared to the detector outputs. In order to increase
the chances of detection these template banks must span a large
multi-dimensional parameter space requiring
significant computational cost. For \ac{GW} bursts that remain un-modelled; there are no
templates available and so matched-filtering is unsuitable for the detection of
these signals.  Instead, detection algorithms like (cWB, x-pipeline, etc... \jordan{reference}) involve distinguishing the signal from
detector noise by looking for excess power contained in the time-frequency
domain and rely on the
astrophysical burst waveform appearing in multiple detectors at similar times.
This is only possible if the detector noise is well characterised and the
candidate signal can be differentiated from systematic or environmental
glitches. 

% Discuss the family of burst waveforms currently used and why - not in detail, just
% an introduction 
%
\ac{GW} burst detection algorithms~\cite{Klimenko_2008, Aso_2008} are tested
and tuned using modelled waveforms that may or may not have astrophysical
significance but have easy to define parameters and share characteristics of
real bursts that are enough to simulate a \ac{GW} passing between
detectors~\chris{long sentence}. Such waveforms include sine-Gaussians: a
Gaussian modulated sine wave that is characterised by its central frequency and
decay parameter. Bandlimited white-noise bursts: white noise that is contained
within a certain frequency range and ring-downs which mimic the damped
oscillations after a \ac{CBC} merger. A Gaussian blip: a short exponential increase the decrease in amplitude and a Binary black hole in spiral. 
% cut this text if not going to be used
%
\begin{comment}Such waveforms may have
long-duration, short bandwidth (ringdowns), long-duration, large bandwidth
(inspirals) and many algorithms make use of sine-Gaussians: a Gaussian
modulated sine wave that is characterised by it's central frequency and narrow
bandwidth.~\chris{not the time to try to describe the types of burst waveforms.
Also, be careful with satying things like ringdowns have narrow bandwidth. I
know we specify a single frequency but becuase of the short duration, the
signal is broad band. Look at the FFT} This makes it a great tool for
diagnosing LIGOs sensitivity to frequency.~\chris{strange unfinished sentence.} 
\end{comment}

% Introduce ML techniques in GWs
%
With the expectation that there will be many more \ac{GW} detections in the
future, there is a growing need for fast and efficient \ac{GW} analysis methods
to match the rising number of detections. While still in its infancy, applying \ac{ML} to \ac{GW} analyses have already shown great potential in
areas of detection~\cite{Gabbard2017,Gebhard_2019,Krastev_2020}, where these techniques have matched the sensitivity of matched filtering for Advanced LIGO and Advanced Virgo gravitational-wave searches. In identifying and classifying detector noise transients or  'glitches"~\cite{Bahaadini, George_2018,Razzano_2018, 2020arXiv200801262G}. In Bayesian parameter
estimation~\cite{gabbard2019bayesian,
green2020gravitationalwave} where \ac{ML} techniques can recover parameters of a \ac{GW} signal significantly faster than standard methods. Long duration signals like continuous \ac{GW} require long observing times and therefore have large amounts of data needing to be processed. Current \ac{ML} approaches~\cite{2020PhRvD.102b2005D, 2019PhRvD.100d4009D, 2020arXiv200708207B} are particularly well suited to dealing with this as once trained the searches can be performed quickly. \jordan{quicker than than what?}

% What this paper does on GANs in 1 paragraph - the point of the paper
%
In this work we aim to explore the use of \ac{ML} to generate and interpret
unmodelled \ac{GW} burst waveforms. Using the generative machine learning
model, \acp{GAN}, we train on five classes of waveforms in the time domain. Working on the assumption that \acp{GAN} construct smooth
 high dimensional vector spaces between their input and output, we can then
explore the space between the five classes to construct new
hybrid waveforms. As all the computationally expensive
processes occur during training, the learned model after training is able to
produce waveforms in fractions of a second and in fact produce waveforms that
are difficult to produce with current
techniques. These new varieties of waveforms can then be used to diagnose
detection algorithms, gain new insight into sources of \ac{GW}
bursts and  allow us to better train our algorithms to a
broader range of possible signals and therefore enhance our detection ability. 

% the structure of the paper
%
This paper is organised as follows: \jordan{unstructured at the
moment}~\chris{time to add the structure.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generative Adversarial Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\begin{itemize}
\item Describe GANs in detail but really focus on the fact that the reader is a
GW data analyst - not a computer scientist \ding{51}
\item A diagram would be very useful \ding{51}
\item Do not discuss our specific case here - just stay general \ding{51}
\item A subsection on the specific advanced flavour of GAN that you are using
here - motivate this choice. \ding{51}
\end{itemize}
\end{comment}

% introduce GANs
%
\subsection{Artificial neural networks}

% inroduce the concept of ML
%

\begin{figure}[h!]
\begin{subfigure}[b]{0.30\textwidth}
   \begin{subfigure}[b]{1\textwidth}
   	\centering
	\resizebox{\textwidth}{!}{
 	\begin{tikzpicture}		
		    \node[scale=0.3] 		   (x1) at (0,0.8) {$x_1$}; 
		    \node[scale=0.3] 		   (x2) at (0,0.5) {$x_2$};  
		    \node[scale=0.3] 		   (x3) at (0,0.2) {$x_3$};       
    		    \node [draw, circle,scale=0.18,fill=blue!25]          (c3) at (0.7,0.5) {\Large $\sigma(\sum_i w_i x_i + b)$};
    \draw[->] (x1)--(c3);
    \draw[->] (x2)--(c3);
    \draw[->] (x3)--(c3);
     \draw[->] (c3)--(1.3,0.5);
   	\end{tikzpicture}
	}
	\caption{}
	 \label{fig:perceptron}
 \end{subfigure}
 
   \begin{subfigure}[b]{1\textwidth}
  	\centering
	\resizebox{\textwidth}{!}{
\begin{tikzpicture}
\begin{axis}[
    xmin=-2.5, xmax=2.5,
    ymin=-1.5, ymax=1.5,
    axis lines=center,
    axis on top=true,
    domain=-2.5:2.5,
    ylabel=$\sigma$,
    xlabel=$x$,
    ]

    \addplot [mark=none,draw=green,ultra thick] {tanh(\x)};
    \addplot+[mark=none,draw=blue,domain=-2.5:0,ultra thick] {0};
    \addplot+[mark=none,draw=blue,domain=0:2.5,ultra thick] {x};
    \addplot+[mark=none,red,dotted,domain=-2:0,ultra thick] {0.2*x};
    \addplot+[mark=none,red,dotted,domain=0:2.5,ultra thick] {x};

    %% Add the asymptotes
\end{axis}
\end{tikzpicture}
	}
	\caption{}
	 \label{fig:activations}
\end{subfigure}
\end{subfigure}
	 \begin{subfigure}[b]{0.70\textwidth}
\centering
\resizebox{\textwidth}{!}{
\def\layersep{1.5cm}
\begin{tikzpicture}[shorten >=1pt,draw=black, ->,node distance=\layersep]


    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,draw=black, very thick, fill=blue!25, minimum size=22pt,inner sep=0pt]
    \tikzstyle{input neuron}=[circle,draw=black,very thick,fill=black!25, minimum size=22pt,inner sep=0pt];
    \tikzstyle{output neuron}=[neuron];
    \tikzstyle{hidden neuron}=[circle,draw=black,very thick,fill=blue!25,minimum size=22pt,inner sep=0pt];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,5}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron] (I-\name) at (0,-\y) {$x_{\name}$};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,3}
        \path[yshift=-1cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
            
  \foreach \namee / \y in {1,...,3}
        \path[yshift=-1 cm]
            node[hidden neuron] (H1-\namee) at (2*\layersep,-\y cm){};

           
    % Draw the output layer node
  \node[output neuron,pin={[pin edge={->}]right:}, right of=H1-2] (O) {};
    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,5}
        \foreach \dest in {1,...,3}
            \path (I-\source) edge (H-\dest);

 \foreach \source in {1,...,3}
        \foreach \dest in {1,...,3}
            \path (H-\source) edge (H1-\dest);
            
    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,3}
        \path (H1-\source) edge (O);

    % Annotate the layers
    %\node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layers};
   % \node[annot,left of=I-4] at (0) {Input layer};
    %\node[annot,right of=hl] {Output layer};
    
    \draw[decoration={brace,mirror,raise=15pt},decorate,-]
  (I-1.north) -- node[left=0.8cm,scale=0.7] {Input layer} (I-5.south);
 
    \draw[decoration={brace,raise=15pt},decorate,-]
  (H-1.west) --  node[above=0.8cm,scale=0.7]{Hidden layers}  (H1-1.east);
  
    \draw[decoration={brace,raise=15pt},decorate,-]
  (O.west) --  node[above=0.8cm,scale=0.7]{Output layers}  (O.east);
 
\end{tikzpicture}
}
 \caption{}
 \label{fig:network}
 \end{subfigure}

\caption{Neural Networks (a) A single neuron taking a vector of inputs and
returning a single output based on the weights, bias and activation function
of the network. (b) A selection of activation functions used in this study. The hyperbolic tangent (Green), Rectified linear unit \cite{relu} (Blue) and Leaky rectified linear unit \cite{Maas2013RectifierNI} (Red). (c)
A an example of a neural network containing two hidden layers that performs a
mapping of an input vector to a single output.}
\end{figure}

\ac{ML} aims to learn apparent relationships held within given data or `training
data' in order to make accurate predictions without the need for additional
programming. A common approach in \ac{ML} relies on the model learning on
past experience to make decisions on future events. Deep learning is a branch of \ac{ML} that takes inspiration from biological processes in the brain. 

% Introduce basic neural networks - a perceptron layer
%
\begin{comment}
Neural networks are the quintessential~\chris{really? quintessential?} \jordan{It's a perfectly cromulent word https://www.imdb.com/title/tt0701155/} \ac{ML}
algorithm that aims to approximate a function. 
\end{comment}

Artificial neural networks are universal function approximators that are built from many single
processing units called neurons. The simplest neural network is the perceptron
layer~\cref{fig:perceptron} which holds a single neuron that takes several real
inputs $x_{i},\ldots, x_{n}$
and maps them to an \jordan{probability?} output according to the linear function, 
%
\begin{align}
f(x) = \sigma(\sum_i w_i x_i + b),
\label{eqn:neuron}
\end{align}
%
where $w$ and $b$ are the weights and bias and $\sigma$
denotes the activation function. The weights are numbers which can be thought
of as the strength between connected neurons. The output of a neuron is defined by its activation function which controls how the neuron 'fires" depending on its input. Some examples of commonly used activation functions are shown in~\cref{fig:activations}. It is often useful to introduce a bias, $b$, such that the neuron remains inactive
above zero but is active when the sum reaches a defined threshold. 

% basic network structure
%
A neural network contains many single neurons connected in a layered structure
as shown in~\cref{fig:network}. The activation's of the first layer (or
input layer) act as the inputs to the second layer and so on until the output
layer. Multi-layered neural networks have intermediate layers between the input
and output stages dubbed the hidden layers.
% the cost or loss function
%
The output of a single neuron is gives a
prediction that can be compared to the real value through a loss (also known as a
cost) function. The network must work to minimise this function by updating the weights in the negative
direction of the loss gradient in a process referred to as gradient
descent \cite{ruder2016overview}. The training process for a single layered network is easy to compute as the weights relate directly to the gradient of the loss function the network is trying to minimise. For deeper architectures, the loss is a complicated function of all the weights in all the layers. The backpropagation \cite{Nielsen1992} algorithm acts over the many paths from node to output. It does so in two phases:

\begin{itemize}
\item Forward phase: For one instance of training, the inputs are fed forward through the network using the current weights and the final output is compared to the training labels. The derivative of the loss function is then computed.
\item Backward phase: This phase learns how the gradient of the loss function changes when the weights are varied. Starting at the output node, the algorithm goes backwards through the network (hence the name). The weights that give the steepest descent to the loss function are saved for the next training instance.  
\end{itemize}
This process of updating the weights is repeated until the loss function reaches convergence or a global minimum. The training data is cycled through in epochs. An epoch is defined when the entire data set is fed through the network. As it is impractical to feed the entire data set into the network at once, so the training is split up into smaller more manageable batches. Finally, the number of batches needed to compete one epoch is referred to as the iterations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convolutional Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\acp{CNN} are designed to work with grid-like input structures that exhibit
strong local spatial dependencies. Although most work with \acp{CNN}
involve image-based data, they can be applied to other spatially adjacent data
types such as time-series and text items. \acp{CNN} are defined by the use of a
convolution operation, a mathematical operation that expresses the amount
overlap between functions. Much like traditional neural networks the convolution operation in this context involves multiplying the input by an array of weights, called a filter or a kernel qhich is typically smaller in size than the input. The convolution is applied by shifting the kernel over the input, drawing out spatially important features between the
two. The distance by which the grid is shifted is known as
the stride and increasing it reduces the dimensionality of the
output, this process is know as down sampling. Alternatively, up sampling the inputs can be achieved using a transposed convolution \cite{dumoulin2016guide}.   The output of the convolutional layer is then passed to an activation function. For deep neural networks, techniques like BatchNormalisation \cite{ioffe2015batch} which standardise the inputs to a layer and SpatialDropout \cite{tompson2014efficient} which sever connections between neurons can both help to stabilise learning.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GANs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% basic intro to GANs
%
A subset of deep learning that has seen fruitful development in recent years
are \acp{GAN}~\cite{Goodfellow2014}. These unsupervised algorithms learn patterns in a
given training data set using an adversarial process. The generations from
\acp{GAN} are currently state-of-the-art in fields such as high quality image
fidelity~\cite{brock2018large,karras2019analyzing}, text-to-image
translation~\cite{reed2016generative}, and video
prediction~\cite{liang2017dual} as well as time series
generations~\cite{esteban2017realvalued}~\chris{do we still not have any other
GW GAN applications to reference or are we hoping to still be the first?}. \jordan{None that i can find?}

% Basic components of a GAN
%
\acp{GAN} train two competing neural networks, consisting of a discriminator
network that is set up to distinguish between real and fake data and a
generator network that produces fake versions of the real data. The generator model performs a mapping from a fixed length vector $\mathbf{z}$ to its
representation of the data. The input vector is drawn randomly from a Gaussian distribution which is referred to as a latent space comprised of latent variables. The latent space is a compressed representation of a data distribution which the generator applies meaning to during training. Sampling points from this space allows the generator to produce a variety of different generations, with different points corresponding to different features in the generations. The discriminator maps its input $\mathbf{x}$ to a probability that the input came from either the training (real) data or
generator (fake).

\begin{comment}
Chris' definition of latent space if needed: It maps the intrinsic
variation of the training space onto the Gaussian latent space distribution.
\end{comment}

% training a GAN
%
During training, the discriminator and generator are updated in parallel using batches of data. Random latent vectors are given to the generator to produce a batch of fake samples and an equal batch of real samples is taken from the training data. The discriminator makes predictions on the real and fake samples and the model is updated through minimising the binary cross-entropy function \cite{Goodfellow-et-al-2016}

\begin{equation}
    L = -y \log(\hat{y}) + (1 - y) \log(1-\hat{y}),
    \label{eqn:crossentropy}
\end{equation}
where $\hat{y}$ is the network prediction and $y$ is the true output. While training the discriminator, D, on real data, we set $y = 1$ and $\hat{y} = D(x)$ which from eqn. \ref{eqn:crossentropy} gives $L(D(x),1) = \log(D(x))$. While training on fake data produced by the generator, G, $y = 0$ and $\hat{y} = D(G(z))$ and so, $L(D(G(z)),0) = \log(1-(D(G(z))))$. \jordan{whole derivation unnecessary?} Since the objective of the discriminator is to correctly classify fake and real data these equations should be maximised, while the goal of the generator should be to minimize these equations. This gives the \ac{GAN} loss as

\begin{equation}
   \mathop{\text{min}}_{G}  \mathop{\text{max}}_{D} V(D,G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{\text{data}}}(\mathbf{x})} [\text{log} D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{\text{z}}(\mathbf{z})} [\text{log}(1-D(G(\mathbf{z})))],
 \label{equation:GANloss}
 \end{equation}
where $p_{\text{\text{data}}}(\mathbf{x})$ is the distribution of real data and $p_{\text{z}}(\mathbf{z})$ is the distribution of the latent space. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conditional GANs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% introduce CGANs and ACGANs
%
To gain more control over what a GAN is able to generate, a conditional variant
of \acp{GAN} named \acp{CGAN}~\cite{cgan} was introduced by feeding in extra
information into the generator and discriminator such as a class label or
attribute label, $c$. This simple addition has shown to work well in practice, for instance in image-to-image translation~\cite{isola2016imagetoimage}. We defined the classes in one-hot encoding framework, that is, each class resides at the corner points of a 5 dimensional cube. For example e.g. $c=[0,1,0,0,0]$ represents the ring-down signal class. The training data and labels are drawn from a joint distribution $p_{data}(\mathbf{x},\mathbf{c})$, whereas when generating fake data we sample from $\mathbf{c}$ and $p_{z}(\mathbf{z})$ independently. \href{equation:GANloss} is modified to include the class labels 
~
\begin{equation}
   \mathop{\text{min}}_{G}  \mathop{\text{max}}_{D} V(D,G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{\text{data}}}(\mathbf{x})} [\text{log} D(\mathbf{x|c})] + \mathbb{E}_{\mathbf{z} \sim p_{\text{z}}(\mathbf{z})} [\text{log}(1-D(G(\mathbf{z|c})))].
 \label{equation:CGANloss}
 \end{equation}
Fig. \ref{fig:gan_comparison} shows the differences in inputs and outputs of a GAN compared with a \ac{CGAN}. We will be using a conditional GAN for this study.

\begin{figure}[h!]
    \begin{subfigure}{.5\textwidth}
     \centering
        \input{figures/GAN_diagram_vertical}
        \caption{GAN}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
     \centering
        \input{figures/ACGAN_diagram_vertical}
        \caption{CGAN}
    \end{subfigure}
    \caption{Comparison of the original GAN method and the
Conditional-GAN method. For CGANs the training data requires a label denoting
its class that is also fed to the generator which then learns to generate
waveforms based on the input label.}
\begin{comment}
~\chris{Overall it's a bit basic and doesn't indicate either the
2 competing training steps or any aspect of the loss or specify that $x$ and
$z$ (and $c$) are drawn from distributions already defined in the text. Maybe it's OK without
that. As for the CGAN, I'm troubled by the fact that there is only one $c$ box.
It implies that the same $c$ value is given to each real and fake $x$ data when
in fact each $x$ sample has it's own randomly drawn $c$ value.}
\end{comment}
\label{fig:gan_comparison}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\begin{itemize}
\item Need to introduce the scheme you propose to use
\item A paragraph or subsection on the data generation being very clear on all
5 waveform models and the prior parameter space for each \ding{51}
\item A subsection on the design of the network architecture \ding{51}
\item A subsection on the "box" and why we implement it \ding{51}
\item A subsection on the training of the network - give rough timings and rule
of thumb decisions made
\item Do not discuss the results here 
\end{itemize}
\end{comment}

% introduce the training data
%

\begin{table}[hb]
\centering
\caption{The parameters used in generating the set of training data. The parameters are drawn uniformly in the below ranges. ~\chris{Add a bit more caption description
here. Plus address the issues of the actual distributions used (uniform) and
how you include the BBH parameters. It's also not obvious that all the classes
share the same 3 parameters.}}
%\footnotesize
\begin{tabular}{@{} l l l l l l }
\br
\hline
 Waveform & Central frequency  & Decay & Central time epoch & Mass range \\
 & (Hz) & (s) & (s) & ($\textrm{M}_{\odot}$) \\
\mr
Sine-Gaussian & 70 - 250 & 0.004 - 0.03 & 0.4 - 0.6 & N/A  \\  
Ringdown & 70 - 250 & 0.004 - 0.03 & 0.4 - 0.6 & N/A \\
White-noise burst & 70 - 250 & 0.004 - 0.03 & 0.4 - 0.6 & N/A  \\
Gaussian pulse & N/A & 0.004 - 0.03 & 0.4 - 0.6 & N/A  \\
BBH & N/A & N/A & N/A & 5 - 70  \\
 \br
\end{tabular}\\
\label{Tab:training_parms}
\end{table}
\normalsize

% introduce the signal models
%
We propose a signal generation scheme using a \ac{CGAN} trained on burst-like
waveforms. We call this \texttt{BurstGAN} \cite{jordan_git} and is a \ac{CGAN} trained on five signal classes each spanning a range
of prior signal parameters. The signal classes are:

% list the 5 waveform classes
%
\begin{itemize}
%
\item {\bf Sine-Gaussian}: $h_{\text{sg}}(t) = A \exp\left[ - (t-t_{0})^2 /
\tau^2 \right] \sin (2 \pi f_0 (t-t_0))$, a sinusoidal wave with a Gaussian
envelope characterised by a central frequency, $f_0$, amplitude, $A$, time of arrival, $t_{0}$. 
%
\item {\bf Ring-down}: $h_{\text{rd}}(t) = A \exp \left[-{(t-t_0)} / {\tau}
\right] \sin(2 \pi f_0 (t-t_0))$, with frequency $f_0$ and duration $\tau$, amplitude, $A$, time of arrival, $t_{0}$. 
%
\item {\bf White-noise bursts}: $h_{\text{wn}}(t_j) = Ag_j\exp\left[ -
(t-t_{0})^2 / \tau^2 \right]$ where $g_j$ are drawn from a zero mean unit
variance Gaussian distribution with a Gaussian envelope of duration $tau$.
%
\item {\bf Gaussian pulse}: $h_{\text{gp}}(t) = \exp(-t^2 / \tau^2)$ with
duration parameter $\tau$.
%
\item {\bf Binary-black hole}: Simulated using the IMRPhenomD
waveform~\cite{Khan_2016} routine from LALSuite~\cite{lalsuite} which models
the inspiral, merger and ringdown of a \ac{BBH} waveform. The component masses
lie in the range of [5,70] $\textrm{M}_{\odot}$ with zero spins and we fix
$m_1>m_2$. The mass distribution is approximated by a power law with
index of 1.6~\cite{Abbott_2019}. The inclinations are drawn
such that the cosine of the angles lies uniformly in the range [-1,1] and we only use the plus polarisation.
%
\end{itemize}
%
The location of the peak amplitude of the waveforms (corresponding to the
mid-points of all but the ring-down and \ac{BBH} classes) are randomly drawn from a uniform distribution to
be within [0.4, 0.6] sec from the start of the 1 sec time interval and all
training waveforms are sampled at 1024 Hz.  The parameter prior ranges are
defined in~\cref{Tab:training_parms} and a sample of training waveforms are shown in \ref{fig:training_waveforms}. All training data is rescaled such that their amplitudes peak at 1.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Paper_draft/figures/training-4.png}
    \caption{Examples of the five different waveforms that were used in training the \ac{GAN} for this study. \jordan{put in the parms here too?}}
    \label{fig:training_waveforms}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Architecture details}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Introduce some architecture features
%
For this work we use a \ac{DCGAN}~\cite{Radford2015} architecture. The generator model is fully convolutional,
upsampled using strided transposed convolutions
with batch normalisation in the
first layer and ReLU activations throughout with the exception of the Tanh activation for
the output layer. The use of Tanh guarantees the output function is bound between [-1,1]. Each transposed
convolutional layer uses a kernel size of 18 and stride of 2. The discriminator network
mirrors that of the generator without batch normalization, using
LeakyReLU activations, SpatialDropout, and a 2-stride convolution for
downsampling. The discriminator output is a single node with Sigmoid activation that can be interpreted as a probability of the the signal being real and both models are trained with binary cross
entropy. The full architecture description can be seen in~\cref{Tab:hyperparameters}.


% describe the hyperparameter tunings
%
Neural networks and subsequently \acp{GAN} have multiple parameters a developer
can tune when designing the model and these are referred to as hyperparameters.
The final network design used in this work comes from the use of trial and
error and the initial designs influenced by the available literature. We found
that the \ac{GAN} performed better with both networks having the same number of
layers and neurons which encourages even
competition between the generator and discriminator.  After tuning the multiple
hyperparameters (see \cref{Tab:hyperparameters}), the \ac{GAN} was trained on
$10^5$ signals  drawn from a categorical
distribution with equal propabilities for each class of
sine-Gaussian,
ring-down, white noise bursts, Gaussian pulse and \acp{BBH}. \jordan{add how many epochs trained for, on what hardware and how long}

As \acp{GAN} are trained by updating one model at the expense of the other, they can be hard to train. The goal of GAN training is to find an equilibrium between the two models, if this cannot be found then it is said that the \ac{GAN} has failed to converge. One way to diagnose \acp{GAN} during the development process is to keep track of the loss over time. Loss plots, for example, as seen in fig. \ref{fig:lossplot} can help to identify common failure modes or to check if the \ac{GAN} has indeed converged. There is currently no notion of early stopping in \acp{GAN}, instead, training is halted after convergence and by visually inspecting the generations. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{Paper_draft/figures/Screenshot 2020-09-11 at 16.52.47.png}
    \caption{Line plots of the discriminator (blue) and generator (orange) loss v iterations. Early in training the losses oscillates as both models attempt to find their equilibrium, after which, both losses vary around 0.7.}
    \label{fig:lossplot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}
\begin{itemize}
\item Begin by outlining the type of results you will be presenting
\item A subsection on the general quality of generated waveforms - we may need
to have overlaps between generated wavefoms and training data (maybe)
\item A subsection on the descriminator - maybe a confusion matrix?
\item a subsection on the latent space varaition within each class - fixed
class, sliding in latent space.
\item A subsection on the class space variation - fixed latent space and
sliding in the class space.
\item A final subsection on the general waveform model based on random latent
and class space locations.
\item Make no conclusions.
\end{itemize}
\end{comment}

% describe the main idea
%
Given a 100-dimensional vector drawn from a normal distributed latent space and a one-hot encoded class
label, the GAN is able to generate burst-like waveforms generalised from the
training set. We set out by describing the quality of generated waveforms and
how they compare to the training set. We then explore the structure of the
latent and class spaces by interpolating between points in these spaces. We
test two methods of sampling from the class space that can be used to generate
a new breed of signal composed of weighted elements of each training class.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/generations/sg.png}
    \includegraphics[width=\textwidth]{figures/generations/rd.png}
    \includegraphics[width=\textwidth]{figures/generations/wnb.png}
    \includegraphics[width=\textwidth]{figures/generations/blip.png}
    \includegraphics[width=\textwidth]{figures/generations/bbh.png}
    \caption{\ac{GAN} Generated waveforms plotted as a function of time. The latent space inputs for each panel is randomised and each row is assigned one of the five class vectors. By row: Sine-Gaussian, Ringdown,
White-noise burst, Gaussian pulse, Binary black hole merger. On the first four rows each x axis is zoomed for ease of viewing, while the last row shows the full one second. \jordan{re-do with latent space all zeros to all ones (see colab)}}
\label{fig:gen_signals} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Known class signal generation}

After training the networks, the generator's function is to map $G$ : $\mathbf{z},\mathbf{c} $ $\in$
$\mathbb{R}^{100}$ $\to$ $\mathbb{R}^{1024}$. The results from the
generator can be seen in \cref{fig:gen_signals}. Each plot shows the output of
the generator for random input latent space vectors $\mathbf{z}$ and each row
corresponds to one of the five class vectors $\mathbf{c}$.~\chris{yes? What
about the plot? Anything positive to say about what you're showing? Please
comment on the quality of the generated waveforms and the good varaition within
each class representative of the variation in the training data. The point is
to show the general variation and quality.}

\chris{You also need to make it clear somewhere (maybe here) that you are
generting single polarisation waveforms with normalised amplitude fixed to
unity.}

\chris{Also, how is the reader going to judge that your waveforms are good if
we don't show any examples of the training data. I recommend a single plot
showing 5 waveforms (5 panels horizontally) for reference.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interpolation within the latent space}

% expain what we are planning to show here
%
In this section we explore the latent space formed by the generator by interpolation. We take two random
points in the latent space and linearly interpolate between them~\chris{OK, but
how do you select the latent points? Are they the same pair of points for each
class?}. These new latent space vectors can now be fed into the generator to
make predictions on~\chris{"to make predictions on" - what does that mean?}
while keeping the class vectors constant at each of the 5 \chris{fiducial?}
training classes. The full effect shown in~\cref{fig:z_interp}~\chris{not a
sentence}. We can see that each plot shows plausible waveforms suggesting that
the generator has constructed a smooth space unlike the discrete training
case~\chris{don't confuse the issue by saying that the class space is discrtee
when later we want to show the opposite}. Additionally as each class is given
the same latent points to interpolate over, we can see that the waveforms
cluster together with respect to their parameters. Visually, the sine-gaussian
and ring-down waveforms share similar frequencies and the other signals show
similar decays and starting epochs for a common latent space input. The only
exception is for the \ac{BBH} waveforms. This can be explained by the fact that
they were trained with more variety of parameters~\chris{what is more variety
of parameters? Does it really mean anything?} and consistently have their peak
amplitudes in the last quarter of the time series~\chris{do they? In the plot
the peak amplitude locations seem to range from 0.6 back to 0.5 but that's not
too different to the ring-downs, the white noise or the pulse. The
sine-Gaussian is the one that stands out as different. Can you please verify
the correctness of the plot you are referring to.}.

\chris{I think that it might be sensible to hand-pick the latent space
locations as the origin and the point $[1,1,\ldots,1]/2$. When you do random
locations we don't know what we should expect to get.}

\ac{ML} algorithms are often described as universal function approximators. In
the case of the generator it maps samples drawn from a 100 dimensional Gaussian
distribution in the latent space to its representation of the training set. As
with any function, there should be a one to one mapping from the domain and
co-domain to allow for smooth transitions across the latent space. One
advantage of using \acp{GAN} as a waveform generator is that once it is
trained, it can perform rapid generations faster than computationally expensive
algorithms \michael{e.g.?}~\chris{you can maybe reference Seb Kahn and Green
here soon. It will be on the arXiv next week I think - they did fast CBC
waveform generation.  Also Walter Del Pozzo has a paper xoming out soon on
this. Neither use GANs though, just ML}. For complicated data sets, the network
architecture must be diverse and dense enough to capture distinct variations
from the training set.  Most GANs perform well on relatively low resolution
image generations, however, higher resolutions demand larger networks and long
training times. GANs attempting to replicate complicated structures and do not
have the necessary architecture either struggle to produce results at all or
fall into the common failure mode know as mode collapse; where the generator
produces a small variety of samples or simply memorises the training set. To
test this, we perform linear interpolations in the latent and class
space.~\chris{lots of technical talk before actually saying what you are trying
to do. Reverse this. Also try to provide references for the technical talk} 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/generations/z_interp_sg.png}
    \includegraphics[width=\textwidth]{figures/generations/z_interp_rd.png}
    \includegraphics[width=\textwidth]{figures/generations/z_interp_wnb.png}
    \includegraphics[width=\textwidth]{figures/generations/z_interp_blip.png}
    \includegraphics[width=\textwidth]{figures/generations/z_interp_bbh.png}
    \caption{Latent space linear interpolation, class space is
constant.~\chris{lots more discription needed. what is being plotted against
what? What is being held fixed, what is being varied.}}
    \label{fig:z_interp}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Interpolation between pairs of classes}

In order to explore the class space we keep the latent vector constant and
interpolate through the 5 classes. We construct a path between the 5 waveforms
and show that the space is populated enough to allow for transitions between
classes. Sine-Gaussian to ringdown performs well in interpolation with each
signal being a plausible burst GW. It is obvious that the GAN has clustered
these two groups during training as they share many characteristics. The other
signals have sharper transitions but still retain plausible looking waveforms. 

~\chris{this is one of  the most important sections of the paper and there
is barely 1 paragrpah about it. Please beter explain what you are doing in
general when you fix the latent space location and explore the class space. The
concept of having arbitrary points in the 5-D class space should have been made
clear earlier on in the CGAN section (or elsewhere). There also isn't currently a
reference to the corresponding figure.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/generations/interp_blip-sg.png}
    \includegraphics[width=\textwidth]{figures/generations/interp_wnb-rd.png}
    \includegraphics[width=\textwidth]{figures/generations/interp_sg-bbh.png}
    \caption{Class space linear interpolation, class space is held
constant~\chris{no it isn't}.
Top row: gaussian blip to sine-Gaussian, middle row: white noise burst to
ringdown, bottom row: sine-Gaussian to BBH.~\chris{Again, more description,
start from the basics, what is plotted against what. Is the latent space fixed
the same for each row? Add the class space vectors on each panel. We have 5
classes and so there are 10 possible pairs but you can't fit 10 rows here so
why do 3? Why not 5 or 6? }}
    \label{fig:c_interp}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General interpolsation within the class space}

So far the analysis has focused~\chris{not really focussed, you only dedicated
1 paragraph to it in the previous subsection} on interpolating between two
classes of signals with the aim to use the interpolated signals as unmodelled
waveforms, instead, we may consider random mixtures of classes~\chris{don't
sell this according to our thought processes as we did the research, sell it as
"we just explained the pair interpolation since it's a stepping stone to the
general interpolation"}. We defined the classes in one-hot encoding framework,
that is, each class resides at the corner points of a 5 dimensional
cube~\chris{this should have already been stated n earlier sections}. In order
to generate an even mixture~\chris{by definition, it won't be an even mixture
if points are randomly sampled} of classes we sample points uniformly in this
space. \jordan{this would be the box}. Alternativly we can sampe points from
the plane that intersects the four corners, which for a 5-dimensional case is,
sample from a 4-simplex.~\chris{please dedicate an entire paragraph to the
simplex and box explanation. You may even need an equation or 2.} This can be
seen in~\ref{fig:unmodelled_samples}. \jordan{why? this enforces the vector
points to sum to one. Why is that good? Well it keeps the points lying on the
simplex.}~\chris{indeed. The reasons were originally that the plane is the
simplest structure (a hyper-surface or manifold if you;re being fancy) in the
class space that intersects all classes and has a symmetry such that no class
is favoured over any other - however that property is largely due to the
locations within the one-hot encoding space. Sampling from the simplex can be
seen as sampling from between the classes - truly interpolating within the
space. Alternatively, the cube is not the simplest space since it contains
points up to a distance of unity from the closst class e.g. [0,0,0,0,0] is a
distance of 1 away from all classes (as is [1,1,1,1,1]). This space contains
the simplex as a subspace but also allows for some level of extrapolation.
Hence we would expect the unexpected in the cube and hope for less surprises
with the simplex. THere is also the possibility of exploring outside the cube.
} 

~\chris{Also, discuss the corresponding figure! There has to be comment on the
features of simplex vs box generations as well as general comments about how
the GAN likes to mix signals up. Pick on specific examples and comment on
interesting or annoying features.}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/generations/simplex_sample1.png}
    \includegraphics[width=\textwidth]{figures/generations/simplex_sample2.png}
    \includegraphics[width=\textwidth]{figures/generations/simplex_sample3.png}
    \includegraphics[width=\textwidth]{figures/generations/uniform_sample3.png}
    \includegraphics[width=\textwidth]{figures/generations/uniform_sample2.png}
    \includegraphics[width=\textwidth]{figures/generations/uniform_sample1.png}
    \caption{GAN generations where the class vectors are sampled from the 4-D
plane and sampled uniformly in the class space.~\chris{As always, far more
description needed. What are you plotting against what? Do you fix or randomise
the latent space locations? You absolutely need to specify which are simplex
and which are box.}}
    \label{fig:unmodelled_samples}
\end{figure}

~\chris{I have an idea for the plots. Could we have a colour coding system
where each class gets a colour and then when you mix them you plot it with the
mixed colours - you average the RGB values. Maybe everything would look brown?
Worth a try?}

% remove these comments
%
\begin{comment}
We can now think of ways to properly sample from the space between these points
in a uniform way. Uniformity enforces the condition that we have a variety of
waveforms. Thge easiest being to uniformly sample points in this space and use
them as the class vectors for the generator. Another way would be to keep the
magnitude of the class vector as 1. For a 5-dimensional space we would sample
from the 4-dimensional plane that intersects the corner points.  \end{comment}

\michael{We don't show any results with two detectors yet we use two detectors
for the CNN, we might want to think when/where it's best to show some
generations with both detectors. Maybe when we show a signal with noise since
it's all part of the post-processing?}~\chris{I agree, see my comment in the
CNN section next.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Searching for generalised burst - a CNN analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section of the analysis we will forensically~\chris{no we won't}
compare different waveform generations from the \ac{CGAN} to help determine the
success and failures of the model~\chris{what does this mean? What model?}. We
investigate a simple case using a \ac{CNN} to detect whether a signal is
contained in noise or if the data contains noise only. The signals we use to
train the \ac{CNN} are generated from the \ac{GAN} with their latent input
randomised from a 100-dimensional Gaussian distribution~\chris{yes, but this
should be obvious since we never deviate from the 100-D latent vector approach}
and are categorised by the method used to sample from the class space. These
methods are divided into the following: 

\begin{itemize}
%
\item {\bf Conditional}: Definite categorical generations~\chris{yes, but what
does that mean. You've never mentioned that phrase before} from the \ac{GAN}.
These signals are the closest to the training set.
%
\item {\bf Uniform}: Generated using a uniform distribution U[0,1] as the input
class vector~\chris{yes, but you need to specify the 5-D nature of the space}.
%
\item {\bf Simplex}: Generated using a Dirichlet distribution that samples from the
4-simplex as the input class vector and includes within it the conditional
locations.~\chris{all fine as long as you have defined the 4-simplex earlier
on.} 
%
\end{itemize}

\chris{please add an example plot showing the normalised and whitened input of
an SNR=8 signal, one panel for each detector, with noisy signal and noise-free
signal plotted on top. Then remember to reference it in the text.}

The CNN~\chris{whoah, hold on. What CNN? You said you were going to use one but
can we get a few more details first. what's the input, what's the output,
what's the loss function, then point us to a table of params in the appendix.}
is trained to distinguish between two classes: signals in additive Gaussian
noise and Gaussian noise, where “signals” are taken from either the
Conditional, Uniform or Simplex cases~\chris{the signals+noise and noise
statement is repetetive of the previous paragraph.}. This means in total there
are three \acp{CNN} to train and all \acp{CNN} share the same architectural
structure. All of the training data used is whitened using the Advanced LIGO
design sensitivity \ac{PSD}, such that there is equal noise power at each
frequency and the data is correctly normalised~\chris{but what is correctly
normalised. Remember trhat we have to be reproducible so you have to make sure
that someone reading this can repeat your experiment.}. This procedure is
applied to signals whose \acp{SNR} are sampled uniformly in the range $[1-16]$.
For each run~\chris{what is a run} the training data consists of $2\times 10^5$
signals which contain one half noise only and one half signal contained in
noise. Of that, 80\% is used for training and 20\% used for validation. A
different testing set is also used in the analysis that is $2\times 10^5$ in
size.~\chris{This final sentence is worded strangely but also why do we have
such a large testing set? I can only imagine that we did this so that we could
go to small $\sim 10^{-5}$ false alarm rates. Is that right?} 

\chris{there is no mention of the "old" box, the one that randomised the sky
location, time shifted the signals and applied the antenna patterns. Also no
mention of multi-detector data and no mention of use of channels. Please add
all of this. }

*table of parameters used for CNN*~\chris{put this in the appendix with the
GAN table}

In~\cref{fig:roc_curves} we compare the \ac{CNN} results between the three
datasets, we train three \acp{CNN} on the Conditional, simplex and uniform data sets
and use these models to make predictions on the other unseen datasets. We make
a comparison by fixing the fraction of samples incorrectly identified as
signals (false alarm rate) and plotting this versus the optimal SNR of the
signals.~\chris{this last sentence isn't quite right} 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/conditional_trained.png}
    \includegraphics[width=0.8\textwidth]{figures/simplex_trained.png}
    \includegraphics[width=0.8\textwidth]{figures/uniform_trained.png}
    \caption{Efficiency curves comparing the performance of \acp{CNN} trained on
conditional generations (top), simplex generations (middle), uniform
generations (bottom) for a fixed false alarm rate of $10^{-3}$.~\chris{more
description. What is plotted against what? The fonts are different to the other
plots. The y-axis is not a rate but a prbability (a FAP unfortunately)}}
\label{fig:roc_curves} 
\end{figure}

These results show for a \ac{CNN} trained on conditional signals, the uniform data
set is distinguishable from the conditional and simplex dataset. The \ac{CNN} is
robust enough to capture the differences between the datasets and shows that
the GAN can generate a variety of unmodelled waveforms that can be used in
future testing. 

~\chris{please write more about the results of the CNN}

% remove this comment if not being used
\begin{comment}
{\subsection{Vector Arithmetic}
In DCGANs the authors demonstrated unsupervised vector arithmetic with celebrity face generation. They kept points in the latent space and performed simple vector arithmetic to generate new images. This allows for intuitive and targeted generation of images. However, the authors realised that single images vectors were unstable and there was a need to average three vectors before any arithmetic. DCGANs is unconditional, therefore, the vectors used were chosen empirically, generating many faces and choosing the attributes for study. Here, we show that this GAN only needs a single vector representation and due to conditioning we can have more control over which vectors to use in the arithmetic. In \cref{fig:arithmetic} we generate a whitenoise burst and a BBH insprial using their respective class labels and use a randomised latent point and response. Keeping the latent point and response, we average the two class vectors and use this as input to the generator. \Cref{fig:arithmetic} (c) shows the generation based on the combined class vectors which visually looks similar to a supernova waveform. Supernova waveform generations require expensive numerical relativity simulations and various assumptions about the collapse of its parent star. Here, we are able to produce similar waves at a fraction of the expense and we are able to further modify the resultant by using different latent space samples. \jordan{this is the adding noise thing but i need to work out the correct way to do it with the directional vector}. 
\end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% summary of the paper
%
In this work we present the potential of \acp{GAN} for
burst \ac{GW} analysis. We have shown that \acp{GAN} have the ability to generate 5
class varieties~\chris{yes, but you make it sound like 5 is special and it's
not} of modelled burst \ac{GW} signals that can be generated at
whim~\chris{really? whim?}. The latent and class spaces were explored through interpolation and
suggest that the space provides smooth translations between classes and overall
waveform shape. We then showed targeted waveform generation by mixing classes
to produce new unmodelled waveform varieties that can be used to test current
burst search pipelines. \jordan{couple of sentences about
classifier}.~\chris{expand this. You have 15-20 pages of material so you can
get a nioce big paragrpah or 2 out of this. It will be repetitive, yes, but
that's the point.} 

% extra things to think about
%
In order to extend this work to a viable burst wave generator and classifier a
few points require further research. In principle it is trivial to add another
detector inside the response layer \jordan{response is the wrong thing to say
since the time delay isnt really a response, extrinsic layer? just non
trainable layer? The box?}, however, as this now 3 dimensional signal is feed
to the discriminator this will no doubt require further tweaking of the
network. We can add more burst-like wave forms in the training set, like
detector glitches which would similarly require further network design. The
work presented here is noise free. To havea complete generation and detection
package we would like to train the network on signals hidden in additive
Gaussian noise and test the ability of the auxiliary classifier. 
~\chris{this paragrph is old and out of date. Please unpdate it.}

The approach shown in this work shows promise in generating unmodelled burst
waveforms from exotic~\chris{no} sources. Having the ability to quickly
generate new waveforms is essential to test current detection schemes and their
susceptibilty to unmodelled sources. We believe that \acp{GAN} have the ability
to generate high fidelity waveforms at a fraction of the computational
expense~\chris{don't focus on the expense part since burst waveforms are cheap
anyway} and do not rely on large prior parameter space. Having banks of these
waveforms at hand can aid in our understand of the physics processes behind
these non-standard ac{GW} emitters.  
~\chris{we need to write down paragrph headinsg in the next meeting. I'm tired.}

\jordan{I want to include a link to the github etc but also a google collab
scrip like the one for BigGANs: \\
\url{https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb.}
It's fun and gives a better feel for the interpolating that static images.}

\begin{comment}
\begin{itemize}
\item Summarise the paper
\item Dedicate a paragraph to each of the key results discussed in the previous
section
\item Have at least one paragraph on the future directions of this work
\item Conclude with a positive paragrpah about the potential uses and impact of
the approach.
\end{itemize}
\end{comment}

\section*{References}
\bibliography{references}

\clearpage

\appendix
\section{List of hyperparameters}
\begin{table}[hb]
\caption{CGAN architecture~\chris{More, more, more desctription. Also split the
generatpr and descriminator parts.}}
\footnotesize
\begin{tabular}{@{}l l l l l l l}
\br
 Operation & Kernel & Strides & Output Shape & BN & Dropout & Activation \\
\mr
 G(\textbf{z}): Input \textbf{z} $\sim$ Normal(0,0.02) & N/A & N/A & (100,) & \ding{55} & 0 & N/A \\  
 Dense & N/A & N/A & (32768,) & \ding{55} & 0 & ReLU \\  
 Class input c & N/A & N/A & (1,) & \ding{55} & 0 & N/A \\
 Embedding & N/A & N/A & (1, 120) & \ding{55} & 0 & N/A \\
 Dense & N/A & N/A & (1,128) & \ding{55} & 0 & ReLU \\ 
 Reshape \textbf{z} & N/A & N/A & (128, 256) & \ding{55} & 0 & N/A \\
 Reshape c & N/A & N/A & (128, 1) & \ding{55} & 0 & N/A \\
 Concatenate & N/A & N/A & (128, 257) & \ding{55} & 0 & N/A \\
 Reshape & N/A & N/A & (64, 514) & \ding{55} & 0 & N/A \\
 Transposed Convolution & 18x1 & 2 & (256, 256) & \ding{51} & 0 & ReLU\\
 Transposed Convolution & 18x1 & 2 & (512, 128) & \ding{55} & 0 & ReLU\\
 Transposed Convolution & 18x1 & 2 & (1024, 64) & \ding{55} & 0 & ReLU\\
 Convolution & 18x1 & 1 & (1024, 1) & \ding{55} & 0 & Tanh \\
 Sky input & N/A & N/A & (3,) & \ding{55} & 0 & N/A \\
 Concatenate & N/A & N/A & (1027,) &  \ding{55} & 0 & N/A \\
 Lambda & N/A & N/A & (1024, 2) & \ding{55} & 0 & N/A \\
 D(\textbf{x}): Input \textbf{x} & N/A & N/A & (1024, 2) & \ding{55} & 0 & N/A \\
 Convolution & 14x1 & 2 & (512, 64) & \ding{55} & 0.5 & Leaky ReLU \\
 Convolution & 14x1 & 2 & (256, 128) & \ding{55} & 0.5 & Leaky ReLU \\
 Convolution & 14x1 & 2 & (128, 256) & \ding{55} & 0.5 & Leaky ReLU \\
 Convolution & 14x1 & 2 & (64, 512) & \ding{55} & 0.5 & Leaky ReLU \\
 Flatten & N/A & N/A & (32768,) & \ding{55} & 0 & N/A \\
 Dense & N/A & N/A & (1,) & \ding{55} & 0 & Sigmoid \\
 Dense & N/A & N/A & (5,) & \ding{55} & 0 & Softmax \\
\br
 Optimizer & \multicolumn{6}{l}{Adam($\alpha$ = 0.0002, $\beta_{1}$ = 0.5)} \\
 Batch size & \multicolumn{6}{l}{128}  \\
 Iterations & \multicolumn{6}{l}{60000}  \\
 Leaky ReLU slope & \multicolumn{6}{l}{0.2} \\
 Weight initialization & \multicolumn{6}{l}{Gaussian($\mu$ = 0, $\sigma$ = 0.02)} \\
 Generator loss & \multicolumn{6}{l}{Binary cross-entropy} \\
 Discriminator loss & \multicolumn{6}{l}{Binary cross-entropy \& sparse categorical cross-entropy} \\ 
 \br
\end{tabular}\\
\label{Tab:hyperparameters}
\end{table}
\normalsize

\section{Many more generated examples}
~\chris{do we still need this?}

\end{document}

