\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{iopart-num_custom}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Abbott2016}
\citation{Prospects-dets}
\citation{AdvLIGO}
\citation{AdvLIGO2}
\citation{AdvVIRGO}
\citation{Abbott2016a}
\citation{Abbott2017}
\citation{Abbott2017a}
\citation{Abbott2017b}
\citation{Owen1998}
\providecommand \oddpage@label [2]{}
\newacro{GW}[GW]{gravitational wave}
\newacro{GAN}[GAN]{generative adversarial network}
\newacro{CGAN}[CGAN]{conditional generative adversarial network}
\newacro{ACGAN}[ACGAN]{auxilliary conditional generative adversarial network}
\newacro{DCGAN}[DCGAN]{deep convolutional generative adversarial network}
\newacro{CNN}[CNN]{convolutional neural networks}
\newacro{BBH}[BBH]{binary black hole}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\AC@undonewlabel{acro:GW}
\newlabel{acro:GW}{{1}{1}{Introduction}{section*.1}{}}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\citation{Klimenko_2008}
\citation{Aso_2008}
\citation{george2016deep}
\citation{Gabbard2017}
\citation{Gebhard_2019}
\citation{Krastev_2020}
\citation{Bahaadini}
\citation{George_2018}
\citation{Razzano_2018}
\citation{gabbard2019bayesian}
\citation{shen2019deterministic}
\citation{green2020gravitationalwave}
\citation{Gabbard2017}
\citation{gabbard2019bayesian}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\acronymused{GW}
\@writefile{toc}{\contentsline {section}{\numberline {2}Generative Adversarial Networks}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Artificial neural networks}{2}{subsection.2.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:perceptron}{{1a}{3}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig:perceptron}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig:tanh_activation}{{1b}{3}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig:tanh_activation}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\newlabel{fig:neural_network}{{1c}{3}{Subfigure 1c}{subfigure.1.3}{}}
\newlabel{sub@fig:neural_network}{{(c)}{c}{Subfigure 1c\relax }{subfigure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neural Networks (a) A single neuron taking a vector of inputs and returning a singular output based on the weights, bias and activation function of the network. (b) The hyperbolic tangent used as an activation function. (c) A fully connected neural network containing two hidden layers that performs a mapping of an input vector to a singular output.\relax }}{3}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}{subfigure.1.3}\protected@file@percent }
\newlabel{eqn:neuron}{{1}{3}{Artificial neural networks}{equation.2.1}{}}
\citation{Goodfellow2014}
\citation{brock2018large}
\citation{karras2019analyzing}
\citation{reed2016generative}
\citation{liang2017dual}
\citation{esteban2017realvalued}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}GANs}{4}{subsection.2.2}\protected@file@percent }
\AC@undonewlabel{acro:GAN}
\newlabel{acro:GAN}{{2.2}{4}{GANs}{section*.3}{}}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\acronymused{GAN}
\newlabel{equation:GANloss}{{2}{4}{GANs}{equation.2.2}{}}
\citation{Nash1950}
\citation{cgan}
\citation{odena2016conditional}
\acronymused{GAN}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Conditional GANs}{5}{subsection.2.3}\protected@file@percent }
\acronymused{GAN}
\AC@undonewlabel{acro:CGAN}
\newlabel{acro:CGAN}{{2.3}{5}{Conditional GANs}{section*.4}{}}
\acronymused{CGAN}
\newlabel{equation:cGANloss}{{3}{5}{Conditional GANs}{equation.2.3}{}}
\AC@undonewlabel{acro:ACGAN}
\newlabel{acro:ACGAN}{{2.3}{5}{Conditional GANs}{section*.5}{}}
\acronymused{ACGAN}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of the original GAN method and the Auxiliary Conditional-GAN method. For ACGANs the training data requires a label denoting its class that is also fed to the generator which then learns to generate waveforms based on the input label. Additionaly, the discrminator learns to classify which class the signal belongs to.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:gan_comparison}{{2}{6}{Comparison of the original GAN method and the Auxiliary Conditional-GAN method. For ACGANs the training data requires a label denoting its class that is also fed to the generator which then learns to generate waveforms based on the input label. Additionaly, the discrminator learns to classify which class the signal belongs to.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{6}{section.3}\protected@file@percent }
\acronymused{GW}
\acronymused{GAN}
\acronymused{GAN}
\newlabel{eqn:sg}{{4}{6}{Methodology}{equation.3.4}{}}
\citation{Radford2015}
\citation{DBLP:journals/corr/abs-1809-11096}
\AC@undonewlabel{acro:BBH}
\newlabel{acro:BBH}{{3}{7}{Methodology}{section*.7}{}}
\acronymused{BBH}
\acronymused{BBH}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Examples of simulated \ac {GW} burst signals. Top row shows examples from the training set. From left to right: Sine-Gaussian, Ringdown, White-noise burst, Gaussian pulse, Binary black hole merger. The bottom row shows the conditional generations from the GAN.\nobreakspace  {}\textbf  {\leavevmode {\color  {green}CHRIS: slightly more explanation. The reader may be concerned that the top and bottom do not look the same. Should they? They may wonder why is there only one waveform and not one per detector?}} \relax }}{8}{figure.caption.8}\protected@file@percent }
\acronymused{GW}
\newlabel{fig:train}{{3}{8}{Examples of simulated \ac {GW} burst signals. Top row shows examples from the training set. From left to right: Sine-Gaussian, Ringdown, White-noise burst, Gaussian pulse, Binary black hole merger. The bottom row shows the conditional generations from the GAN.~\chris {slightly more explanation. The reader may be concerned that the top and bottom do not look the same. Should they? They may wonder why is there only one waveform and not one per detector?} \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Architecture details}{8}{subsection.3.1}\protected@file@percent }
\acronymused{GAN}
\AC@undonewlabel{acro:DCGAN}
\newlabel{acro:DCGAN}{{3.1}{8}{Architecture details}{section*.9}{}}
\acronymused{DCGAN}
\acronymused{GAN}
\acronymused{GAN}
\AC@undonewlabel{acro:CNN}
\newlabel{acro:CNN}{{3.1}{8}{Architecture details}{section*.10}{}}
\acronymused{CNN}
\acronymused{CNN}
\acronymused{GAN}
\citation{lalsuite}
\acronymused{GAN}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Class labels and embedding}{9}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Applying a time shift and antenna responses}{10}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{10}{section.4}\protected@file@percent }
\acronymused{GW}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Waveform quality}{11}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Interpolation}{11}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Latent space interpolation}{11}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Generated waveforms after training and conditioning on five classes. The generator will output two waveforms as seen by detectors in Hanford (red) and Livingston (blue). The generator is able to capture the characteristics of each waveform and structure the class space to give control over which waveform to generate. Each row shows a random assortment of one of the five classes the GAN is trained on. \relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:gen_signals}{{4}{12}{Generated waveforms after training and conditioning on five classes. The generator will output two waveforms as seen by detectors in Hanford (red) and Livingston (blue). The generator is able to capture the characteristics of each waveform and structure the class space to give control over which waveform to generate. Each row shows a random assortment of one of the five classes the GAN is trained on. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Class space interpolation}{12}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Interpolations between two random latent points in z. Each row uniformly interpolates between two points in $\mathbf  {z}$ keeping the class fixed. Only a single waveform from the generator is plotted and each signal is re-scaled to [-1,1] effectively removing the antenna responses for clarity.\relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:z_interp}{{5}{13}{Interpolations between two random latent points in z. Each row uniformly interpolates between two points in $\mathbf {z}$ keeping the class fixed. Only a single waveform from the generator is plotted and each signal is re-scaled to [-1,1] effectively removing the antenna responses for clarity.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Vector Arithmetic}{13}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Class space interpolation with latent space held constant throughout. The plots show a zoomed in section of the 1s time interval. Top row: Sine-Gaussian class to ringdown class, Middle Row: Sine-Gaussian class to whitenoise burst class, Bottom row: Sine-Gaussian class to Gaussian class.\relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:c_interp}{{6}{14}{Class space interpolation with latent space held constant throughout. The plots show a zoomed in section of the 1s time interval. Top row: Sine-Gaussian class to ringdown class, Middle Row: Sine-Gaussian class to whitenoise burst class, Bottom row: Sine-Gaussian class to Gaussian class.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Class space interpolation with latent space held constant throughout. The full 1s interval is plotted for class based interpolations between a Sine-Gaussian and a BBH in spiral.\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig:sgbbh_interp}{{7}{14}{Class space interpolation with latent space held constant throughout. The full 1s interval is plotted for class based interpolations between a Sine-Gaussian and a BBH in spiral.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Conditional vector arithmetic. (a) Generated samples of a whitenoise burst and BBH insprial. (c) Effect of naively adding the components from (a) in the time domain. \textbf  {\leavevmode {\color  {red}JORDAN: just adding the two signals post generation}} (c) Generation from the combined class vectors. \relax }}{15}{figure.caption.15}\protected@file@percent }
\newlabel{fig:arithmetic}{{8}{15}{Conditional vector arithmetic. (a) Generated samples of a whitenoise burst and BBH insprial. (c) Effect of naively adding the components from (a) in the time domain. \jordan {just adding the two signals post generation} (c) Generation from the combined class vectors. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Classifier}{15}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{15}{section.5}\protected@file@percent }
\acronymused{GW}
\bibdata{iopart-num}
\bibcite{Abbott2016}{1}
\bibcite{Prospects-dets}{2}
\bibcite{AdvLIGO}{3}
\bibcite{AdvLIGO2}{4}
\bibcite{AdvVIRGO}{5}
\bibcite{Abbott2016a}{6}
\bibcite{Abbott2017}{7}
\bibcite{Abbott2017a}{8}
\bibcite{Abbott2017b}{9}
\bibcite{Owen1998}{10}
\bibcite{Klimenko_2008}{11}
\bibcite{Aso_2008}{12}
\bibcite{Gabbard2017}{13}
\bibcite{gabbard2019bayesian}{14}
\bibcite{Goodfellow2014}{15}
\bibcite{brock2018large}{16}
\bibcite{karras2019analyzing}{17}
\bibcite{reed2016generative}{18}
\bibcite{liang2017dual}{19}
\bibcite{esteban2017realvalued}{20}
\bibcite{Nash1950}{21}
\bibcite{cgan}{22}
\bibcite{odena2016conditional}{23}
\bibcite{Radford2015}{24}
\bibcite{DBLP:journals/corr/abs-1809-11096}{25}
\bibcite{lalsuite}{26}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix A}List of hyperparameters}{18}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {A1}{\ignorespaces ACGAN architecture\relax }}{18}{table.caption.17}\protected@file@percent }
\newlabel{Tab:hyperparameters}{{A1}{18}{ACGAN architecture\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix B}Many more generated examples}{18}{appendix.B}\protected@file@percent }
